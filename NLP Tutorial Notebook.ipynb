{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# author: Shril Kumar [shril.iitdhn@gmail.com]\n",
    "\n",
    "# Tokenization - process of converting a text into tokens\n",
    "# Tokens       - words or entities present in the text (entities: Paragraph, Sentences)\n",
    "# Text object  – a sentence or a phrase or a word or an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "# 1. Noise Removal - Stopwords, URLs, Punctuations, mentions etc.\n",
    "# 2. Lexicon Normalization - Tokenization, Normalization, Stemming\n",
    "# 3. Object Standardization - Regular Expressions, Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem fake news important should be taken care as soon as possible\n"
     ]
    }
   ],
   "source": [
    "# Noise Removal\n",
    "\n",
    "input_text = \"the problem of fake news is important and it should be taken care of as soon as possible\"\n",
    "noise_list = [\"is\", \"a\", \"the\", \"and\", \"of\", \"it\"]\n",
    "\n",
    "def remove_noise(text):\n",
    "    words = text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_list]\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text\n",
    "\n",
    "print(remove_noise(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing:  run\n",
      "Stemming   :  run\n",
      "Lemmatizing:  lie\n",
      "Stemming   :  lie\n",
      "Lemmatizing:  carry\n",
      "Stemming   :  carri\n"
     ]
    }
   ],
   "source": [
    "# Lexicon Normalization Practices\n",
    "# 1. Stemming - Removing suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "# 2. Lemmatization - step by step procedure of obtaining the root form of the word\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "words = [\"running\",\"lying\",\"carrying\"]\n",
    "for w in words:\n",
    "    print(\"Lemmatizing: \",  lem.lemmatize(w, \"v\")) # 2nd argument means POS of word \n",
    "    print(\"Stemming   : \",  stem.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct message me this is a Retweet tweet by Shril\n"
     ]
    }
   ],
   "source": [
    "# Object Standardization\n",
    "# Text data often contains words or phrases which are not present in any standard lexical dictionaries.\n",
    "# Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs.\n",
    "\n",
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', 'awsm' : 'awesome', 'luv' :'love'}\n",
    "\n",
    "def lookup_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "    new_text = \" \".join(new_words)\n",
    "    return new_text\n",
    "\n",
    "print(lookup_words(\"DM me this is a rt tweet by Shril\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRP')\n",
      "('am', 'VBP')\n",
      "('learning', 'VBG')\n",
      "('Natural', 'NNP')\n",
      "('Language', 'NNP')\n",
      "('Processing', 'NNP')\n",
      "('on', 'IN')\n",
      "('Analytics', 'NNP')\n",
      "('Vidhya', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "# Text to Features\n",
    "# POS = Parts of Speech\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for pos in pos_tag(tokens):\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.029*\"driving\" + 0.029*\"father\" + 0.029*\"practice.\" + 0.029*\"lot\" + 0.029*\"time\" + 0.029*\"spends\" + 0.029*\"of\" + 0.029*\"a\" + 0.029*\"dance\" + 0.029*\"around\"'), (1, '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"stress\" + 0.060*\"pressure.\" + 0.060*\"may\"'), (2, '0.083*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"my\" + 0.033*\"is\" + 0.033*\"sugar,\" + 0.033*\"not\" + 0.033*\"Sugar\" + 0.033*\"consume.\" + 0.033*\"but\"')]\n"
     ]
    }
   ],
   "source": [
    "# Entity Detection Methods\n",
    "# 1. Named Entity Recognition (NER)\n",
    "# 2. Topic Modeling\n",
    "\n",
    "# Following is the code to implement topic modeling using LDA in python\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]\n"
     ]
    }
   ],
   "source": [
    "# N-Grams as Features\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "print(generate_ngrams('this is a sample text', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.58448290102\n",
      "  (0, 2)\t0.58448290102\n",
      "  (0, 4)\t0.444514311537\n",
      "  (0, 1)\t0.345205016865\n",
      "  (1, 1)\t0.385371627466\n",
      "  (1, 0)\t0.652490884513\n",
      "  (1, 3)\t0.652490884513\n",
      "  (2, 4)\t0.444514311537\n",
      "  (2, 1)\t0.345205016865\n",
      "  (2, 6)\t0.58448290102\n",
      "  (2, 5)\t0.58448290102\n"
     ]
    }
   ],
   "source": [
    "# Statistical Features\n",
    "# TF-IDF Link: https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# The model creates a vocabulary dictionary and assigns an index to each word.\n",
    "# Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       1.00      0.67      0.80         3\n",
      "    Class_B       0.75      1.00      0.86         3\n",
      "\n",
      "avg / total       0.88      0.83      0.83         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Classification\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')\n",
    "                  ]\n",
    "test_corpus =  [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')\n",
    "               ]\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "    \n",
    "test_data = [] \n",
    "test_labels = []\n",
    "\n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "    \n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels)\n",
    "\n",
    "prediction = model.predict(test_vectors)\n",
    "print(classification_report(test_labels, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
